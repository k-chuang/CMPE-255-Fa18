{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Nearest Neighbors using LSH\n",
    "\n",
    "This notebook will show the give and take between building multiple hash tables and employing multiple hash functions when searching for approximate neighbors using LSH. The demonstration is meant to clarify slide 12 from slide deck 3, on Nearest Neighbors.\n",
    "\n",
    "I have written a basic LSH implementation in Python, with instantiations for the cosine similarity, Hamming distance, and the Jaccard coefficient LSH families. The code is written in OOP style and can be easily extended to other LSH families. Take a look at lsh.py for the details. In this demo, we will be using the cosine similarity version of the LSH data structure, *clsh*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:53:55.018477Z",
     "start_time": "2018-09-29T01:53:54.362624Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "import numpy as np\n",
    "from lsh import clsh, jlsh, generateSamples, findNeighborsBrute, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will generate some random samples, and split the data into train (X) and test (Y) subsets. Samples are generated from 100 gausian blobs, i.e., points will be fairly spread out as far as their cosine similarity is concerned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:53:55.369688Z",
     "start_time": "2018-09-29T01:53:55.346967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 100) (100, 100)\n"
     ]
    }
   ],
   "source": [
    "X, Y = generateSamples(nsamples=1000, nfeatures=100, nclusters=64, clusterstd=50, binary=False)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic concept in LSH is that of *hashing* the vectors using a random LSH family of hash functions. As we discussed in class, the LSH families will be more likely to assign the same hash value to similar items. This, however, does not happen all the time. First, let's see what the result of hashing a vector looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:53:56.128403Z",
     "start_time": "2018-09-29T01:53:56.104823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "L11 = clsh(X, ntables=1, nfunctions=1)\n",
    "for i in range(10):\n",
    "    print(L11.hash(X[i,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in slide 18, the output of the cosine family of LSH function is binary, depending on the sign of the dot-product $\\langle r,x\\rangle$ between the random unit vector $r$ and our input vector $x=X[i,:]$.\n",
    "\n",
    "Note that we created a single table in our LSH data structure and are using a single LSH function to hash vectors. This means that we're simply partitioning vectors into two buckets. Some vectors will go to the bucket with ID 0, and others will go to the bucket with ID 1.\n",
    "\n",
    "When we instantiated the LSH data structure *L*, all the vectors in X were already assigned to their respective buckets. Let's see how many vectors each bucket has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:53:56.594739Z",
     "start_time": "2018-09-29T01:53:56.577073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket ID 0 has 459 vectors.\n",
      "Bucket ID 1 has 441 vectors.\n"
     ]
    }
   ],
   "source": [
    "print(\"Bucket ID 0 has %d vectors.\" % len(L11.tables[0]['0']))\n",
    "print(\"Bucket ID 1 has %d vectors.\" % len(L11.tables[0]['1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when it's time to find neighbors for a new vector, say $y=Y[0,:]$, the first vector in our test set, we hash the vector to see which bucket we should look in to find neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:53:57.067252Z",
     "start_time": "2018-09-29T01:53:57.049917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(L11.hash(Y[0,:], tid=0, fid=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I passed in the ID of the table I'm searching in and the ID of the function I'm hashing with. For LSH to work, we have to use the same hashing functions that were used to create the table(s). Therefore, $clsh$ stores the randomly generated functions it created for each table.\n",
    "\n",
    "Now, it looks like I have to compare $y$ against almost half of the vectors in $X$, which is a lot, and leads to low *precision*. Precision is the fraction of retrieved instances (the vectors we compared against) that are relevant (that would also be in the exact result). Since the number of objects we're comparing against is high, precision will be low. In order to increase the precision, I can use several hash functions and concatenate their results. Increasing the precision will also reduc the amount of time spent finding neighbors, as we will have fewer objects to compare against.\n",
    "\n",
    "Let's say I use 2 hash functions from the Cosine LSH family. Then, the possible resulting hash values would be 00, 01, 10, and 11, spliting the vectors in X into 4 buckets (instead of 2, when we used 1 function). If we use 3 functions, we get 8 buckets. In general, using $f$ functions will split the \"search space\" into $2^f$ buckets.\n",
    "\n",
    "Let's try this using 3 functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:53:57.621135Z",
     "start_time": "2018-09-29T01:53:57.593565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket ID 000 has 109 vectors.\n",
      "Bucket ID 011 has 135 vectors.\n",
      "Bucket ID 010 has 97 vectors.\n",
      "Bucket ID 111 has 125 vectors.\n",
      "Bucket ID 101 has 99 vectors.\n",
      "Bucket ID 110 has 95 vectors.\n",
      "Bucket ID 001 has 106 vectors.\n",
      "Bucket ID 100 has 134 vectors.\n",
      "\n",
      "We only need to compare y against vectors in bucket 100.\n"
     ]
    }
   ],
   "source": [
    "L13 = clsh(X, ntables=1, nfunctions=3)\n",
    "for k in L13.tables[0].keys():\n",
    "    print(\"Bucket ID %s has %d vectors.\" % (k, len(L13.tables[0][k])))\n",
    "    \n",
    "print(\"\\nWe only need to compare y against vectors in bucket %s.\" % L13.signature(Y[0,:], tid=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side note**: Note that in this academic LSH implementation we use a simple way to generate bucket IDs. We concatenate the string representation of the resulting hash value from each hash function. LSH libraries often implement a secondary (exact) hash function for generating numeric IDs for the buckets. A similar scheme is proposed in the LSH reference I nored on Canvas: [SPM'08] Malcolm Slaney and Michael Casey. Locality-Sensitive Hashing for Finding Nearest Neighbors. Lecture Notes. IEEE Signal Processing Magazine, 2008.\n",
    "\n",
    "It is easy to see we now have much fewer vectors to compare against when we search for $y$'s neighbors. However, some of the true neighbors may have been accidentally placed in other buckets, which lowers *recall*. Recall (also known in Statistics references as *sensitivity*) is the fraction of relevant instances that are retrieved, i.e., the fraction of true neighbors in our top-$k$ divided by $k$. \n",
    "\n",
    "Let's compare the mean recall for finding neighbors using 1 hash function vs. 3 hash functions. To do that, we will first have to find the \"true neighbors\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:54:00.216400Z",
     "start_time": "2018-09-29T01:53:58.864148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of computed similarities for the brute-force approach: 90000.\n",
      "Recall with 1 hash function: 0.550700. Number of computed similarities: 45072.\n",
      "Recall with 3 hash functions: 0.174300. Number of computed similarities: 11354.\n"
     ]
    }
   ],
   "source": [
    "k = 100  # number of neighbors to find\n",
    "nbrsExact = findNeighborsBrute(X, Y, k=k, sim=\"cos\")\n",
    "print(\"Number of computed similarities for the brute-force approach: %d.\" % (X.shape[0] * Y.shape[0]))\n",
    "nbrsTest11  = L11.findNeighbors(Y, k=k)\n",
    "nbrsTest13  = L13.findNeighbors(Y, k=k)\n",
    "print(\"Recall with 1 hash function: %f. Number of computed similarities: %d.\" % (recall(nbrsTest11, nbrsExact), L11.nsims))\n",
    "print(\"Recall with 3 hash functions: %f. Number of computed similarities: %d.\" % (recall(nbrsTest13, nbrsExact), L13.nsims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase the recall by building several LSH tables instead of one. Then, instead of looking in one bucket for $y$'s neighbors, we will be looking in one bucket in each table. The search method gets the set union of object IDs in all these buckets, and then computes similarities against all of them.\n",
    "\n",
    "### Excercise 1\n",
    "\n",
    "Compare the mean recall for finding neighbors using 1 table vs. 3 tables, when each table uses 3 hash functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:54:00.264066Z",
     "start_time": "2018-09-29T01:54:00.218789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket ID 111 has 125 vectors.\n",
      "Bucket ID 110 has 95 vectors.\n",
      "Bucket ID 101 has 99 vectors.\n",
      "Bucket ID 010 has 97 vectors.\n",
      "Bucket ID 000 has 109 vectors.\n",
      "Bucket ID 001 has 106 vectors.\n",
      "Bucket ID 100 has 134 vectors.\n",
      "Bucket ID 011 has 135 vectors.\n"
     ]
    }
   ],
   "source": [
    "L33 = clsh(X, ntables=3, nfunctions=3)\n",
    "for k in L33.tables[0].keys():\n",
    "    print(\"Bucket ID %s has %d vectors.\" % (k, len(L13.tables[0][k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:54:01.729757Z",
     "start_time": "2018-09-29T01:54:00.535217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of computed similarities for the brute-force approach: 90000.\n",
      "Recall with 1 table with 3 hash function: 0.174300. Number of computed similarities: 11354.\n",
      "Recall with 3 table with 3 hash function: 0.429300. Number of computed similarities: 30188.\n"
     ]
    }
   ],
   "source": [
    "k = 100  # number of neighbors to find\n",
    "nbrsExact = findNeighborsBrute(X, Y, k=k, sim=\"cos\")\n",
    "print(\"Number of computed similarities for the brute-force approach: %d.\" % (X.shape[0] * Y.shape[0]))\n",
    "nbrsTest13  = L13.findNeighbors(Y, k=k)\n",
    "nbrsTest33  = L33.findNeighbors(Y, k=k)\n",
    "print(\"Recall with 1 table with 3 hash function: %f. Number of computed similarities: %d.\" % (recall(nbrsTest13, nbrsExact), L13.nsims))\n",
    "print(\"Recall with 3 table with 3 hash function: %f. Number of computed similarities: %d.\" % (recall(nbrsTest33, nbrsExact), L33.nsims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given high enough # tables and # hashes (hash functions), we can achieve high recall and precision, sometimes at the expense of efficiency.\n",
    "\n",
    "### Excercise 2\n",
    "\n",
    "Find the minimum number of tables necessary to obtain `recall` of at least `0.90` using 3 functions. What is the number of computed similarities for that LSH forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:54:10.308686Z",
     "start_time": "2018-09-29T01:54:03.355376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of optimal tables is 13, with recall: 0.905,  number of computed similiarities: 73492\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "nbrsExact = findNeighborsBrute(X, Y, k=k, sim=\"cos\")\n",
    "\n",
    "for i in np.arange(1, 25):\n",
    "    L = clsh(X, ntables=i, nfunctions=3)\n",
    "    nbrsTest  = L.findNeighbors(Y, k=k)\n",
    "    r = recall(nbrsTest, nbrsExact)\n",
    "    if r >= 0.90:\n",
    "        optimal_nsims = L.nsims\n",
    "        optimal_tables = i\n",
    "        optimal_recall = r\n",
    "        break\n",
    "\n",
    "print('Number of optimal tables is %d, with recall: %.03f,  number of computed similiarities: %d' \n",
    "      % (optimal_tables, optimal_recall, optimal_nsims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Repeat Exrcise 1 and 2 using `Jaccard Coefficient` instead of `cosine similarity`. Note that you will need to re-generate samples using the `binary=True` parameter and re-compute `nbrsExact` for the new similarity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:54:18.944663Z",
     "start_time": "2018-09-29T01:54:18.921093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 100) (100, 100)\n"
     ]
    }
   ],
   "source": [
    "X, Y = generateSamples(nsamples=1000, nfeatures=100, nclusters=64, clusterstd=50, binary=True)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:54:19.610543Z",
     "start_time": "2018-09-29T01:54:19.566234Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "\n",
    "JL13 = jlsh(X, ntables=1, nfunctions=3)\n",
    "JL33 = jlsh(X, ntables=3, nfunctions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T01:54:22.121163Z",
     "start_time": "2018-09-29T01:54:20.251852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of computed similarities for the brute-force approach: 90000.\n",
      "Recall with 1 table with 3 hash function: 0.086700. Number of computed similarities: 4285.\n",
      "Recall with 3 table with 3 hash function: 0.224700. Number of computed similarities: 11155.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k = 100  # number of neighbors to find\n",
    "nbrsExact = findNeighborsBrute(X, Y, k=k, sim=\"jac\")\n",
    "print(\"Number of computed similarities for the brute-force approach: %d.\" % (X.shape[0] * Y.shape[0]))\n",
    "nbrsTest13  = JL13.findNeighbors(Y, k=k)\n",
    "nbrsTest33  = JL33.findNeighbors(Y, k=k)\n",
    "print(\"Recall with 1 table with 3 hash function: %f. Number of computed similarities: %d.\" % \n",
    "      (recall(nbrsTest13, nbrsExact), JL13.nsims))\n",
    "print(\"Recall with 3 table with 3 hash function: %f. Number of computed similarities: %d.\" % \n",
    "      (recall(nbrsTest33, nbrsExact), JL33.nsims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T02:03:40.745970Z",
     "start_time": "2018-09-29T02:03:15.347198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of optimal tables is 32, with recall: 0.908,  number of computed similiarities: 62869\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "\n",
    "for i in np.arange(1, 100):\n",
    "    J = jlsh(X, ntables=i, nfunctions=3)\n",
    "    nbrsTest  = J.findNeighbors(Y, k=k)\n",
    "    r = recall(nbrsTest, nbrsExact)\n",
    "    if r >= 0.90:\n",
    "        jopt_nsims = J.nsims\n",
    "        jopt_tables = i\n",
    "        jopt_recall = r\n",
    "        break\n",
    "\n",
    "print('Number of optimal tables is %d, with recall: %.03f,  number of computed similiarities: %d' \n",
    "      % (jopt_tables, jopt_recall, jopt_nsims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- Mean Recall of at least 0.90, given three functions:\n",
    "    - Cosine:\n",
    "        - 13 tables with 73492 computed similiarities\n",
    "    - Jaccard:\n",
    "        - 32 tables with 62869 computed similiarities\n",
    "- Jaccard took more tables but less computation, while cosine took less tables but more computation\n",
    "    - **Tradeoff between memory / computational power**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
